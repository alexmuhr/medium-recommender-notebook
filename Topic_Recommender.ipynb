{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audioVersionDurationSec</th>\n",
       "      <th>codeBlock</th>\n",
       "      <th>codeBlockCount</th>\n",
       "      <th>collectionId</th>\n",
       "      <th>createdDate</th>\n",
       "      <th>createdDatetime</th>\n",
       "      <th>firstPublishedDate</th>\n",
       "      <th>firstPublishedDatetime</th>\n",
       "      <th>imageCount</th>\n",
       "      <th>isSubscriptionLocked</th>\n",
       "      <th>...</th>\n",
       "      <th>slug</th>\n",
       "      <th>name</th>\n",
       "      <th>postCount</th>\n",
       "      <th>author</th>\n",
       "      <th>bio</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>usersFollowedByCount</th>\n",
       "      <th>usersFollowedCount</th>\n",
       "      <th>scrappedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>265164.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>5708.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>it</td>\n",
       "      <td>It</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:04:37</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:06:29</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>technology</td>\n",
       "      <td>Technology</td>\n",
       "      <td>166125.0</td>\n",
       "      <td>George Sykes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93b9e94f08ca</td>\n",
       "      <td>tasty231</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:04:37</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:06:29</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>robotics</td>\n",
       "      <td>Robotics</td>\n",
       "      <td>9103.0</td>\n",
       "      <td>George Sykes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93b9e94f08ca</td>\n",
       "      <td>tasty231</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   audioVersionDurationSec codeBlock  codeBlockCount  collectionId  \\\n",
       "0                        0       NaN             0.0  638f418c8464   \n",
       "1                        0       NaN             0.0  638f418c8464   \n",
       "2                        0       NaN             0.0  638f418c8464   \n",
       "3                        0       NaN             0.0           NaN   \n",
       "4                        0       NaN             0.0           NaN   \n",
       "\n",
       "  createdDate      createdDatetime firstPublishedDate firstPublishedDatetime  \\\n",
       "0  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "1  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "2  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "3  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n",
       "4  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n",
       "\n",
       "   imageCount  isSubscriptionLocked  ...        slug        name postCount  \\\n",
       "0           1                 False  ...  blockchain  Blockchain  265164.0   \n",
       "1           1                 False  ...     samsung     Samsung    5708.0   \n",
       "2           1                 False  ...          it          It    3720.0   \n",
       "3          13                 False  ...  technology  Technology  166125.0   \n",
       "4          13                 False  ...    robotics    Robotics    9103.0   \n",
       "\n",
       "         author  bio        userId    userName  usersFollowedByCount  \\\n",
       "0   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "1   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "2   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "3  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n",
       "4  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n",
       "\n",
       "   usersFollowedCount scrappedDate  \n",
       "0               404.0     20181104  \n",
       "1               404.0     20181104  \n",
       "2               404.0     20181104  \n",
       "3                22.0     20181104  \n",
       "4                22.0     20181104  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium = pd.read_csv('Medium_AggregatedData.csv')\n",
    "medium.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "There is so much redundant info in this dataset. Kill it. Kill it with fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24576, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>allTags</th>\n",
       "      <th>readingTime</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Business, Government and Blockchain</td>\n",
       "      <td>https://medium.com/s/story/private-business-go...</td>\n",
       "      <td>[Blockchain, Samsung, It]</td>\n",
       "      <td>0.958491</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>Private Business, Government and Blockchain\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can a robot love us better than another human ...</td>\n",
       "      <td>https://medium.com/s/story/can-a-robot-love-us...</td>\n",
       "      <td>[Robotics, Meditation, Therapy, Artificial Int...</td>\n",
       "      <td>0.652830</td>\n",
       "      <td>Stewart Alsop</td>\n",
       "      <td>Can a robot love us better than another human ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017 Big Data, AI and IOT Use Cases</td>\n",
       "      <td>https://medium.com/s/story/2017-big-data-ai-an...</td>\n",
       "      <td>[Artificial Intelligence, Data Science, Big Da...</td>\n",
       "      <td>7.055031</td>\n",
       "      <td>Melody Ucros</td>\n",
       "      <td>2017 Big Data, AI and IOT Use Cases\\nAn Active...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Meta Model and Meta Meta-Model of Deep Lea...</td>\n",
       "      <td>https://medium.com/s/story/the-meta-model-and-...</td>\n",
       "      <td>[Machine Learning, Deep Learning, Artificial I...</td>\n",
       "      <td>5.684906</td>\n",
       "      <td>Carlos E. Perez</td>\n",
       "      <td>The Meta Model and Meta Meta-Model of Deep Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Don’t trust “Do you trust this computer”</td>\n",
       "      <td>https://medium.com/s/story/dont-trust-do-you-t...</td>\n",
       "      <td>[Artificial Intelligence, Ethics, Elon Musk, D...</td>\n",
       "      <td>2.739623</td>\n",
       "      <td>Virginia Dignum</td>\n",
       "      <td>Don’t trust “Do you trust this computer”\\nfrom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        Private Business, Government and Blockchain   \n",
       "1  Can a robot love us better than another human ...   \n",
       "2                2017 Big Data, AI and IOT Use Cases   \n",
       "3  The Meta Model and Meta Meta-Model of Deep Lea...   \n",
       "4           Don’t trust “Do you trust this computer”   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://medium.com/s/story/private-business-go...   \n",
       "1  https://medium.com/s/story/can-a-robot-love-us...   \n",
       "2  https://medium.com/s/story/2017-big-data-ai-an...   \n",
       "3  https://medium.com/s/story/the-meta-model-and-...   \n",
       "4  https://medium.com/s/story/dont-trust-do-you-t...   \n",
       "\n",
       "                                             allTags  readingTime  \\\n",
       "0                          [Blockchain, Samsung, It]     0.958491   \n",
       "1  [Robotics, Meditation, Therapy, Artificial Int...     0.652830   \n",
       "2  [Artificial Intelligence, Data Science, Big Da...     7.055031   \n",
       "3  [Machine Learning, Deep Learning, Artificial I...     5.684906   \n",
       "4  [Artificial Intelligence, Ethics, Elon Musk, D...     2.739623   \n",
       "\n",
       "            author                                               text  \n",
       "0      Anar Babaev  Private Business, Government and Blockchain\\n\\...  \n",
       "1    Stewart Alsop  Can a robot love us better than another human ...  \n",
       "2     Melody Ucros  2017 Big Data, AI and IOT Use Cases\\nAn Active...  \n",
       "3  Carlos E. Perez  The Meta Model and Meta Meta-Model of Deep Lea...  \n",
       "4  Virginia Dignum  Don’t trust “Do you trust this computer”\\nfrom...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium = medium[medium['language'] == 'en']         # English Only\n",
    "medium = medium[medium['totalClapCount'] >= 25]     # Posts with > 25 claps\n",
    "\n",
    "def findTags(title):\n",
    "    '''\n",
    "    Function extracts tags for an input title\n",
    "    '''\n",
    "    rows = medium[medium['title'] == title]\n",
    "    tags = list(rows['tag_name'].values)\n",
    "    return tags\n",
    "\n",
    "titles = medium['title'].unique()                   # Get all the titles\n",
    "\n",
    "tag_dict = {'title': [], 'tags': []}               # Dictionary to store tags\n",
    "\n",
    "for title in titles:\n",
    "    tag_dict['title'].append(title)\n",
    "    tag_dict['tags'].append(findTags(title))\n",
    "\n",
    "tag_df = pd.DataFrame(tag_dict)                     # Dictionary to dataframe\n",
    "\n",
    "# Now that tag data is extracted the duplicate rows can be dropped\n",
    "medium = medium.drop_duplicates(subset = 'title', keep = 'first')\n",
    "\n",
    "def addTags(title):\n",
    "    '''\n",
    "    Adds tags back into medium dataframe as a list\n",
    "    '''\n",
    "    try:\n",
    "        tags = list(tag_df[tag_df['title'] == title]['tags'])[0]\n",
    "    except:\n",
    "        # If there's an error assume no tags\n",
    "        tags = np.NaN\n",
    "    return tags\n",
    "\n",
    "# Apply addTags\n",
    "medium['allTags'] = medium['title'].apply(addTags)\n",
    "\n",
    "# Keep only the columns we're interested in for this project\n",
    "keep_cols = ['title', 'url', 'allTags', 'readingTime', 'author', 'text']\n",
    "medium = medium[keep_cols]\n",
    "\n",
    "# Drop row with null title\n",
    "null_title = medium[medium['title'].isna()].index\n",
    "medium.drop(index = null_title, inplace = True)\n",
    "\n",
    "medium.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(medium.shape)\n",
    "medium.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Eliminates links, non alphanumerics, and punctuation. Returns lower case text.\n",
    "    '''\n",
    "    \n",
    "    # Remove links\n",
    "    text = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+','', text)\n",
    "    # Remove non-alphanumerics\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    # Remove punctuation and lowercase\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text.lower())\n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean the text\n",
    "medium['text'] = medium['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = STOPWORDS.union(set(['data', 'ai', 'learning', 'time', 'machine', 'like', 'use', 'new', 'intelligence', 'need', \"it's\", 'way',\n",
    "                                 'artificial', 'based', 'want', 'know', 'learn', \"don't\", 'things', 'lot', \"let's\", 'model', 'input',\n",
    "                                 'output', 'train', 'training', 'trained', 'it', 'we', 'don', 'you', 'ce', 'hasn', 'sa', 'do', 'som',\n",
    "                                 'can']))\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    clean_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stop_list and (len(word) > 2):\n",
    "            clean_text.append(word)\n",
    "    return ' '.join(clean_text)\n",
    "\n",
    "medium['text'] = medium['text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemmer to processedText\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    word_list = []\n",
    "    for word in text.split(' '):\n",
    "        word_list.append(stemmer.stem(word))\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "medium['text'] = medium['text'].apply(stem_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Current State\n",
    "\n",
    "Stemming takes forever, so let's save our progress so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium.to_csv('pre-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium = pd.read_csv('pre-processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TFIDF and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['let'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stop_list, ngram_range = (1,1))\n",
    "doc_word = vectorizer.fit_transform(medium['text'])\n",
    "\n",
    "svd = TruncatedSVD(8)\n",
    "docs_svd = svd.fit_transform(doc_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Display Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "human, network, imag, technolog, work, user, algorithm, predict, peopl, compani, product, busi, deep, custom, develop\n",
      "\n",
      "Topic  2\n",
      "imag, layer, network, neural, function, dataset, featur, weight, convolut, vector, valu, gradient, deep, predict, paramet\n",
      "\n",
      "Topic  3\n",
      "chatbot, bot, user, custom, convers, app, messag, messeng, chat, servic, text, word, voic, assist, interact\n",
      "\n",
      "Topic  4\n",
      "imag, network, layer, neural, human, convolut, deep, chatbot, robot, neuron, technolog, cnn, brain, gan, architectur\n",
      "\n",
      "Topic  5\n",
      "imag, blockchain, tensorflow, file, python, project, api, cloud, instal, token, app, platform, code, team, googl\n",
      "\n",
      "Topic  6\n",
      "blockchain, market, valu, token, custom, layer, network, function, predict, gradient, busi, price, platform, compani, decentr\n",
      "\n",
      "Topic  7\n",
      "scienc, network, chatbot, neural, deep, scientist, layer, cours, python, neuron, gradient, skill, team, function, program\n",
      "\n",
      "Topic  8\n",
      "word, vector, blockchain, text, token, sentenc, languag, embed, network, document, nlp, neural, rnn, matrix, sequenc\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words, no_top_topics, topic_names=None):\n",
    "    count = 0\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if count == no_top_topics:\n",
    "            break\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", (ix + 1))\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        count += 1\n",
    "\n",
    "display_topics(svd, vectorizer.get_feature_names(), 15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "human, robot, technolog, peopl, machin, world, think, futur, brain, job, car, autom, design, game, live\n",
      "\n",
      "Topic  2\n",
      "valu, predict, variabl, featur, regress, function, algorithm, linear, set, test, dataset, paramet, gradient, tree, distribut\n",
      "\n",
      "Topic  3\n",
      "chatbot, bot, custom, user, convers, messag, chat, servic, messeng, busi, assist, app, interact, voic, answer\n",
      "\n",
      "Topic  4\n",
      "network, layer, imag, neural, deep, convolut, neuron, weight, cnn, function, architectur, loss, gener, gan, gradient\n",
      "\n",
      "Topic  5\n",
      "file, tensorflow, imag, python, code, instal, api, run, notebook, googl, librari, creat, app, dataset, gpu\n",
      "\n",
      "Topic  6\n",
      "blockchain, market, technolog, compani, busi, custom, product, platform, servic, token, develop, industri, user, invest, team\n",
      "\n",
      "Topic  7\n",
      "scienc, scientist, cours, work, skill, team, job, peopl, project, engin, busi, analyt, program, deep, start\n",
      "\n",
      "Topic  8\n",
      "word, vector, text, sentenc, embed, languag, document, sentiment, nlp, corpu, sequenc, token, context, topic, matrix\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(8)\n",
    "docs_nmf = nmf.fit_transform(doc_word)\n",
    "\n",
    "display_topics(nmf, vectorizer.get_feature_names(), 15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.034*\"imag\" + 0.024*\"network\" + 0.018*\"layer\" + 0.012*\"neural\" + 0.008*\"deep\" + 0.007*\"dataset\" + 0.007*\"convolut\" + 0.007*\"detect\" + 0.007*\"function\" + 0.007*\"object\"'),\n",
       " (1,\n",
       "  '0.010*\"user\" + 0.010*\"code\" + 0.008*\"python\" + 0.007*\"run\" + 0.007*\"file\" + 0.007*\"app\" + 0.006*\"api\" + 0.006*\"build\" + 0.005*\"project\" + 0.005*\"tool\"'),\n",
       " (2,\n",
       "  '0.008*\"custom\" + 0.008*\"user\" + 0.008*\"human\" + 0.007*\"product\" + 0.007*\"chatbot\" + 0.007*\"scienc\" + 0.007*\"peopl\" + 0.007*\"busi\" + 0.006*\"design\" + 0.006*\"experi\"'),\n",
       " (3,\n",
       "  '0.009*\"human\" + 0.008*\"peopl\" + 0.006*\"world\" + 0.005*\"think\" + 0.005*\"year\" + 0.005*\"robot\" + 0.004*\"technolog\" + 0.004*\"don\" + 0.004*\"day\" + 0.003*\"car\"'),\n",
       " (4,\n",
       "  '0.014*\"valu\" + 0.011*\"function\" + 0.011*\"predict\" + 0.010*\"algorithm\" + 0.007*\"problem\" + 0.007*\"probabl\" + 0.006*\"distribut\" + 0.006*\"optim\" + 0.006*\"state\" + 0.006*\"result\"'),\n",
       " (5,\n",
       "  '0.013*\"featur\" + 0.013*\"word\" + 0.010*\"valu\" + 0.008*\"dataset\" + 0.008*\"number\" + 0.007*\"vector\" + 0.007*\"predict\" + 0.006*\"variabl\" + 0.006*\"let\" + 0.006*\"column\"'),\n",
       " (6,\n",
       "  '0.014*\"deep\" + 0.010*\"network\" + 0.009*\"research\" + 0.009*\"neural\" + 0.008*\"human\" + 0.007*\"languag\" + 0.006*\"algorithm\" + 0.005*\"model\" + 0.005*\"task\" + 0.005*\"comput\"'),\n",
       " (7,\n",
       "  '0.010*\"technolog\" + 0.009*\"compani\" + 0.008*\"market\" + 0.007*\"blockchain\" + 0.006*\"busi\" + 0.006*\"platform\" + 0.005*\"industri\" + 0.005*\"product\" + 0.005*\"servic\" + 0.004*\"team\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = medium['text'].apply(simple_preprocess)\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Workers = 4 activates all four cores of my CPU, \n",
    "lda = models.LdaMulticore(corpus=corpus, num_topics=8, id2word=dictionary, passes=10, workers = 4)\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save NMF Topics\n",
    "And concatenate topic data back to other metadata. Also remove articles with all 0 topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names for dataframe\n",
    "column_names = ['title', 'url', 'allTags', 'readingTime', 'author', 'Tech',\n",
    "                'Modeling', 'Chatbots', 'Deep Learning', 'Coding', 'Business',\n",
    "                'Careers', 'NLP', 'sum']\n",
    "\n",
    "# Create topic sum for each article. Later remove all articles with sum 0.\n",
    "topic_sum = pd.DataFrame(np.sum(docs_nmf, axis = 1))\n",
    "\n",
    "# Turn our docs_nmf array into a data frame\n",
    "doc_topic_df = pd.DataFrame(data = docs_nmf)\n",
    "\n",
    "# Merge all of our article metadata and name columns\n",
    "doc_topic_df = pd.concat([medium[['title', 'url', 'allTags', 'readingTime', 'author']], doc_topic_df, topic_sum], axis = 1)\n",
    "\n",
    "doc_topic_df.columns = column_names\n",
    "\n",
    "# Remove articles with topic sum = 0, then drop sum column\n",
    "doc_topic_df = doc_topic_df[doc_topic_df['sum'] != 0]\n",
    "\n",
    "doc_topic_df.drop(columns = 'sum', inplace = True)\n",
    "\n",
    "# Reset index then save\n",
    "doc_topic_df.reset_index(drop = True, inplace = True)\n",
    "doc_topic_df.to_csv('tfidf_nmf_8topics.csv', index = False)\n",
    "doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_topic_df = pd.read_csv('tfidf_nmf_8topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = ['Tech', 'Modeling', 'Chatbots', 'Deep Learning', 'Coding', 'Business', 'Careers', 'NLP']\n",
    "topic_array = np.array(doc_topic_df[topic_names])\n",
    "norms = np.linalg.norm(topic_array, axis = 1)\n",
    "\n",
    "def compute_dists(top_vec, topic_array):\n",
    "    '''\n",
    "    Returns cosine distances for top_vec compared to every article\n",
    "    '''\n",
    "    dots = np.matmul(topic_array, top_vec)\n",
    "    input_norm = np.linalg.norm(top_vec)\n",
    "    co_dists = dots / (input_norm * norms)\n",
    "    return co_dists\n",
    "\n",
    "def produce_rec(top_vec, topic_array, doc_topic_df, rand = 15):\n",
    "    '''\n",
    "    Produces a recommendation based on cosine distance.\n",
    "    Rand variable controls level of randomness in output recommendation.\n",
    "    '''\n",
    "    # Add a bit of randomness to top_vec\n",
    "    top_vec = top_vec + np.random.rand(8,)/(np.linalg.norm(top_vec)) * rand\n",
    "    co_dists = compute_dists(top_vec, topic_array)\n",
    "    return doc_topic_df.loc[np.argmax(co_dists)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Against User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            Can Brain Activity Predict Consumers’ Preferen...\n",
       "url              https://medium.com/s/story/can-brain-activity-...\n",
       "allTags          [Neuroscience, Behavioral Economics, Consumer ...\n",
       "readingTime                                                4.90503\n",
       "author                                                 Looxid Labs\n",
       "Tech                                                     0.0171501\n",
       "Modeling                                                 0.0239323\n",
       "Chatbots                                                         0\n",
       "Deep Learning                                           0.00719324\n",
       "Coding                                                           0\n",
       "Business                                                 0.0227948\n",
       "Careers                                                          0\n",
       "NLP                                                     0.00131324\n",
       "Name: 12370, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech = 5\n",
    "modeling = 5\n",
    "chatbots = 0\n",
    "deep = 0\n",
    "coding = 0\n",
    "business = 5\n",
    "careers = 0\n",
    "nlp = 0\n",
    "\n",
    "top_vec = np.array([tech, modeling, chatbots, deep, coding, business, careers, nlp])\n",
    "\n",
    "rec = produce_rec(top_vec, topic_array, doc_topic_df)\n",
    "rec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
